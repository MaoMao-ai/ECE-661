{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T17:33:31.065239Z",
     "start_time": "2025-02-18T17:33:31.063105Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 177
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T17:33:32.264891Z",
     "start_time": "2025-02-18T17:33:32.255271Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom Swish activation function with inplace option\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(Swish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(torch.sigmoid(x)) if self.inplace else x * torch.sigmoid(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.swish = Swish(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.down_bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.silu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            identity = self.down_bn(identity)  \n",
    "        out += identity\n",
    "        out = F.silu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetCIFAR(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNetCIFAR, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        #self.swish = Swish(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 32, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 128, layers[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128 * block.expansion, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu', a=0.1)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.silu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define ResNet-20 model with optional num_classes\n",
    "def resnet20(num_classes=10):\n",
    "    return ResNetCIFAR(BasicBlock, [3, 3, 3], num_classes=num_classes)\n"
   ],
   "outputs": [],
   "execution_count": 178
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T14:45:03.829437Z",
     "start_time": "2025-02-18T14:45:03.825566Z"
    }
   },
   "source": [
    "# useful libraries\n",
    "#############################################\n",
    "# your code here\n",
    "# specify preprocessing function\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define data augmentation for training\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 增强颜色\n",
    "    transforms.RandomRotation(15),  # 旋转 15 度\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "])\n",
    "\n",
    "\n",
    "# No augmentation for validation\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "#############################################"
   ],
   "outputs": [],
   "execution_count": 171
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T14:45:06.788445Z",
     "start_time": "2025-02-18T14:45:05.660893Z"
    }
   },
   "source": [
    "# do NOT change these\n",
    "from tools.dataset import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# a few arguments, do NOT change these\n",
    "DATA_ROOT = \"./data\"\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 100\n",
    "\n",
    "#############################################\n",
    "# your code here\n",
    "# construct dataset\n",
    "train_set = CIFAR10(\n",
    "    root=DATA_ROOT, \n",
    "    mode='train', \n",
    "    download=True,\n",
    "    transform=transform_train    # your code\n",
    ")\n",
    "val_set = CIFAR10(\n",
    "    root=DATA_ROOT, \n",
    "    mode='val', \n",
    "    download=True,\n",
    "    transform=transform_val    # your code\n",
    ")\n",
    "\n",
    "# construct dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=TRAIN_BATCH_SIZE,  # your code\n",
    "    shuffle=True,     # your code\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=VAL_BATCH_SIZE,  # your code\n",
    "    shuffle=False,     # your code\n",
    "    num_workers=4\n",
    ")\n",
    "#############################################"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\cifar10_trainval_F22.zip\n",
      "Extracting ./data\\cifar10_trainval_F22.zip to ./data\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data\\cifar10_trainval_F22.zip\n",
      "Extracting ./data\\cifar10_trainval_F22.zip to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T14:45:08.322441Z",
     "start_time": "2025-02-18T14:45:08.305408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# specify the device for computation\n",
    "#############################################\n",
    "# your code here\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device =='cuda':\n",
    "    print(f\"Run on GPU...\\n{torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Run on CPU...\")\n",
    "\n",
    "# Model Definition  \n",
    "net = resnet20()\n",
    "net = net.to(device)\n",
    "    \n",
    "#############################################"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU...\n",
      "NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T14:45:10.069474Z",
     "start_time": "2025-02-18T14:45:10.064800Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# hyperparameters, do NOT change right now\n",
    "# initial learning rate\n",
    "INITIAL_LR = 0.01\n",
    "\n",
    "# momentum for optimizer\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# L2 regularization strength\n",
    "REG = 1e-4\n",
    "\n",
    "#############################################\n",
    "# your code here\n",
    "# create loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Add optimizer\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=INITIAL_LR,\n",
    "    momentum=MOMENTUM,\n",
    "    nesterov=True\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=200)\n",
    "#############################################"
   ],
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T17:28:00.193461Z",
     "start_time": "2025-02-18T17:11:58.799066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# some hyperparameters\n",
    "# total number of training epochs\n",
    "EPOCHS = 50\n",
    "\n",
    "# the folder where the trained model is saved\n",
    "CHECKPOINT_FOLDER = \"./saved_model\"\n",
    "\n",
    "# start the training/validation process\n",
    "# the process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "best_val_acc = 0\n",
    "current_learning_rate = INITIAL_LR\n",
    "L1_lambda = 1e-4\n",
    "\n",
    "\n",
    "print(\"==> Training starts!\")\n",
    "print(\"=\"*50)\n",
    "for i in range(0, EPOCHS):    \n",
    "    #######################\n",
    "    # your code here\n",
    "    # switch to train mode\n",
    "    net.train()\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    print(\"Epoch %d:\" %i)\n",
    "    # this help you compute the training accuracy\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0 # track training loss if you want\n",
    "    \n",
    "    # Train the model for 1 epoch.\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        ####################################\n",
    "        # your code here\n",
    "        # copy inputs to device\n",
    "        inputs, targets = inputs.to(device), targets.to(device).long()\n",
    "        \n",
    "        # compute the output and loss\n",
    "        outputs = net(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute loss with logits\n",
    "        \n",
    "        # Add L1 penalty (sum of absolute values of all model parameters)\n",
    "        l1_penalty = 0\n",
    "        for param in net.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))  # Sum of absolute values\n",
    "        \n",
    "        loss = loss + L1_lambda * l1_penalty\n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # apply gradient and update the weights\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() # Update the training loss\n",
    "        \n",
    "        # count the number of correctly predicted samples in the current batch\n",
    "        _, predicted = outputs.max(1)  # Get predicted classes\n",
    "        total_examples += targets.size(0)  # Increment total examples\n",
    "        correct_examples += predicted.eq(targets).sum().item()  # Increment correct predictions\n",
    "        ####################################\n",
    "                \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "\n",
    "    # Validate on the validation dataset\n",
    "    #######################\n",
    "    # your code here\n",
    "    # switch to eval mode\n",
    "    net.eval()\n",
    "    \n",
    "    #######################\n",
    "\n",
    "    # this help you compute the validation accuracy\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    val_loss = 0 # again, track the validation loss if you want\n",
    "\n",
    "    # disable gradient during validation, which can save GPU memory\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            ####################################\n",
    "            # your code here\n",
    "            # copy inputs to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device).long()\n",
    "            \n",
    "            # compute the output and loss\n",
    "            outputs = net(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute loss with logits\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # count the number of correctly predicted samples in the current batch\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_examples += targets.size(0)\n",
    "            correct_examples += predicted.eq(targets).sum().item()\n",
    "            ####################################\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    \n",
    "    # save the model checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "            os.makedirs(CHECKPOINT_FOLDER)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'state_dict': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'resnet.pth'))\n",
    "        \n",
    "    print('')\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training starts!\n",
      "==================================================\n",
      "Epoch 0:\n",
      "Training loss: 0.9104, Training accuracy: 0.8777\n",
      "Validation loss: 0.3759, Validation accuracy: 0.8806\n",
      "Saving ...\n",
      "\n",
      "Epoch 1:\n",
      "Training loss: 0.9039, Training accuracy: 0.8792\n",
      "Validation loss: 0.3610, Validation accuracy: 0.8836\n",
      "Saving ...\n",
      "\n",
      "Epoch 2:\n",
      "Training loss: 0.9029, Training accuracy: 0.8806\n",
      "Validation loss: 0.3540, Validation accuracy: 0.8796\n",
      "\n",
      "Epoch 3:\n",
      "Training loss: 0.9007, Training accuracy: 0.8788\n",
      "Validation loss: 0.3447, Validation accuracy: 0.8820\n",
      "\n",
      "Epoch 4:\n",
      "Training loss: 0.8996, Training accuracy: 0.8795\n",
      "Validation loss: 0.3446, Validation accuracy: 0.8810\n",
      "\n",
      "Epoch 5:\n",
      "Training loss: 0.8982, Training accuracy: 0.8795\n",
      "Validation loss: 0.3339, Validation accuracy: 0.8876\n",
      "Saving ...\n",
      "\n",
      "Epoch 6:\n",
      "Training loss: 0.9077, Training accuracy: 0.8765\n",
      "Validation loss: 0.3435, Validation accuracy: 0.8852\n",
      "\n",
      "Epoch 7:\n",
      "Training loss: 0.9010, Training accuracy: 0.8799\n",
      "Validation loss: 0.4102, Validation accuracy: 0.8654\n",
      "\n",
      "Epoch 8:\n",
      "Training loss: 0.8915, Training accuracy: 0.8814\n",
      "Validation loss: 0.3093, Validation accuracy: 0.8956\n",
      "Saving ...\n",
      "\n",
      "Epoch 9:\n",
      "Training loss: 0.9012, Training accuracy: 0.8788\n",
      "Validation loss: 0.3704, Validation accuracy: 0.8826\n",
      "\n",
      "Epoch 10:\n",
      "Training loss: 0.8989, Training accuracy: 0.8795\n",
      "Validation loss: 0.3343, Validation accuracy: 0.8900\n",
      "\n",
      "Epoch 11:\n",
      "Training loss: 0.8964, Training accuracy: 0.8787\n",
      "Validation loss: 0.3330, Validation accuracy: 0.8854\n",
      "\n",
      "Epoch 12:\n",
      "Training loss: 0.8929, Training accuracy: 0.8807\n",
      "Validation loss: 0.3354, Validation accuracy: 0.8936\n",
      "\n",
      "Epoch 13:\n",
      "Training loss: 0.8954, Training accuracy: 0.8795\n",
      "Validation loss: 0.3068, Validation accuracy: 0.8926\n",
      "\n",
      "Epoch 14:\n",
      "Training loss: 0.8929, Training accuracy: 0.8801\n",
      "Validation loss: 0.3751, Validation accuracy: 0.8786\n",
      "\n",
      "Epoch 15:\n",
      "Training loss: 0.8929, Training accuracy: 0.8793\n",
      "Validation loss: 0.3605, Validation accuracy: 0.8800\n",
      "\n",
      "Epoch 16:\n",
      "Training loss: 0.8943, Training accuracy: 0.8807\n",
      "Validation loss: 0.3864, Validation accuracy: 0.8754\n",
      "\n",
      "Epoch 17:\n",
      "Training loss: 0.8946, Training accuracy: 0.8797\n",
      "Validation loss: 0.3231, Validation accuracy: 0.8924\n",
      "\n",
      "Epoch 18:\n",
      "Training loss: 0.8926, Training accuracy: 0.8805\n",
      "Validation loss: 0.3469, Validation accuracy: 0.8806\n",
      "\n",
      "Epoch 19:\n",
      "Training loss: 0.8902, Training accuracy: 0.8795\n",
      "Validation loss: 0.3028, Validation accuracy: 0.8978\n",
      "Saving ...\n",
      "\n",
      "Epoch 20:\n",
      "Training loss: 0.8905, Training accuracy: 0.8809\n",
      "Validation loss: 0.3711, Validation accuracy: 0.8778\n",
      "\n",
      "Epoch 21:\n",
      "Training loss: 0.8853, Training accuracy: 0.8806\n",
      "Validation loss: 0.3415, Validation accuracy: 0.8800\n",
      "\n",
      "Epoch 22:\n",
      "Training loss: 0.8926, Training accuracy: 0.8796\n",
      "Validation loss: 0.3271, Validation accuracy: 0.8906\n",
      "\n",
      "Epoch 23:\n",
      "Training loss: 0.8889, Training accuracy: 0.8816\n",
      "Validation loss: 0.3311, Validation accuracy: 0.8912\n",
      "\n",
      "Epoch 24:\n",
      "Training loss: 0.8875, Training accuracy: 0.8789\n",
      "Validation loss: 0.3306, Validation accuracy: 0.8906\n",
      "\n",
      "Epoch 25:\n",
      "Training loss: 0.8885, Training accuracy: 0.8800\n",
      "Validation loss: 0.3223, Validation accuracy: 0.8924\n",
      "\n",
      "Epoch 26:\n",
      "Training loss: 0.8769, Training accuracy: 0.8839\n",
      "Validation loss: 0.3891, Validation accuracy: 0.8736\n",
      "\n",
      "Epoch 27:\n",
      "Training loss: 0.8798, Training accuracy: 0.8834\n",
      "Validation loss: 0.3416, Validation accuracy: 0.8870\n",
      "\n",
      "Epoch 28:\n",
      "Training loss: 0.8837, Training accuracy: 0.8792\n",
      "Validation loss: 0.3550, Validation accuracy: 0.8868\n",
      "\n",
      "Epoch 29:\n",
      "Training loss: 0.8801, Training accuracy: 0.8832\n",
      "Validation loss: 0.3386, Validation accuracy: 0.8876\n",
      "\n",
      "Epoch 30:\n",
      "Training loss: 0.8777, Training accuracy: 0.8833\n",
      "Validation loss: 0.3150, Validation accuracy: 0.8926\n",
      "\n",
      "Epoch 31:\n",
      "Training loss: 0.8810, Training accuracy: 0.8815\n",
      "Validation loss: 0.3218, Validation accuracy: 0.8952\n",
      "\n",
      "Epoch 32:\n",
      "Training loss: 0.8759, Training accuracy: 0.8820\n",
      "Validation loss: 0.3325, Validation accuracy: 0.8846\n",
      "\n",
      "Epoch 33:\n",
      "Training loss: 0.8822, Training accuracy: 0.8805\n",
      "Validation loss: 0.3867, Validation accuracy: 0.8690\n",
      "\n",
      "Epoch 34:\n",
      "Training loss: 0.8828, Training accuracy: 0.8807\n",
      "Validation loss: 0.3307, Validation accuracy: 0.8866\n",
      "\n",
      "Epoch 35:\n",
      "Training loss: 0.8739, Training accuracy: 0.8819\n",
      "Validation loss: 0.3028, Validation accuracy: 0.8962\n",
      "\n",
      "Epoch 36:\n",
      "Training loss: 0.8745, Training accuracy: 0.8816\n",
      "Validation loss: 0.3474, Validation accuracy: 0.8902\n",
      "\n",
      "Epoch 37:\n",
      "Training loss: 0.8723, Training accuracy: 0.8815\n",
      "Validation loss: 0.3252, Validation accuracy: 0.8910\n",
      "\n",
      "Epoch 38:\n",
      "Training loss: 0.8737, Training accuracy: 0.8822\n",
      "Validation loss: 0.3414, Validation accuracy: 0.8870\n",
      "\n",
      "Epoch 39:\n",
      "Training loss: 0.8684, Training accuracy: 0.8838\n",
      "Validation loss: 0.3456, Validation accuracy: 0.8826\n",
      "\n",
      "Epoch 40:\n",
      "Training loss: 0.8763, Training accuracy: 0.8798\n",
      "Validation loss: 0.2942, Validation accuracy: 0.9032\n",
      "Saving ...\n",
      "\n",
      "Epoch 41:\n",
      "Training loss: 0.8722, Training accuracy: 0.8821\n",
      "Validation loss: 0.3068, Validation accuracy: 0.8928\n",
      "\n",
      "Epoch 42:\n",
      "Training loss: 0.8709, Training accuracy: 0.8827\n",
      "Validation loss: 0.3172, Validation accuracy: 0.8954\n",
      "\n",
      "Epoch 43:\n",
      "Training loss: 0.8672, Training accuracy: 0.8824\n",
      "Validation loss: 0.2974, Validation accuracy: 0.8994\n",
      "\n",
      "Epoch 44:\n",
      "Training loss: 0.8674, Training accuracy: 0.8837\n",
      "Validation loss: 0.3300, Validation accuracy: 0.8878\n",
      "\n",
      "Epoch 45:\n",
      "Training loss: 0.8711, Training accuracy: 0.8813\n",
      "Validation loss: 0.3353, Validation accuracy: 0.8860\n",
      "\n",
      "Epoch 46:\n",
      "Training loss: 0.8687, Training accuracy: 0.8822\n",
      "Validation loss: 0.3179, Validation accuracy: 0.8922\n",
      "\n",
      "Epoch 47:\n",
      "Training loss: 0.8657, Training accuracy: 0.8822\n",
      "Validation loss: 0.3285, Validation accuracy: 0.8956\n",
      "\n",
      "Epoch 48:\n",
      "Training loss: 0.8680, Training accuracy: 0.8813\n",
      "Validation loss: 0.3239, Validation accuracy: 0.8932\n",
      "\n",
      "Epoch 49:\n",
      "Training loss: 0.8718, Training accuracy: 0.8804\n",
      "Validation loss: 0.3032, Validation accuracy: 0.8980\n",
      "\n",
      "==================================================\n",
      "==> Optimization finished! Best validation accuracy: 0.9032\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
