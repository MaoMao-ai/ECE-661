{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune GPT-2 on wiki-text\n",
    "\n",
    "In this Lab, we are using a series of library from Hugging Face (i.e. tranformers, datasets, peft). You may need to go through the document of these library to learn the usage. (Hint: you may use the imported contents in the code cell below, other contents is not necessary for this lab)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 3,
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda3\\envs\\torch\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: datasets in d:\\anaconda3\\envs\\torch\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: peft in d:\\anaconda3\\envs\\torch\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (4.49.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (1.4.0)\n",
      "Requirement already satisfied: safetensors in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from peft) (0.29.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2024.2.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# for google colab\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 4,
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
<<<<<<< HEAD
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
=======
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(a) Generate text with GPT2\n",
    "\n",
    "Using the API provided by hugging face, we can easily load the pre-trained GPT2 model and generate text. (GPT2 is a early generative model, the quality of the generated text is not as good as the later model like GPT3.)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 5,
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a langugae model based on transformer developed by OpenAI. It is a simple, fast, and scalable model of the human brain. It is based on the concept of the \"brain as a machine\".\n",
      "\n",
      "The model is based on the concept of the \"brain as a machine\". The model is based on the concept of the \"brain as a machine\". The model is based on the concept of the \"brain as a machine\". The model is based on the\n"
     ]
    }
   ],
   "source": [
    "# your code here: load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length):\n",
    "\n",
    "\n",
    "    # your code here: tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    # your code here: generate token using the model\n",
    "    gen_tokens =  model.generate(input_ids, attention_mask=attention_mask, max_length=max_length)\n",
    "\n",
    "    # your code here: decode the generated tokens\n",
    "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "    print(gen_text)\n",
    "\n",
    "generate_text(model, tokenizer, \"GPT-2 is a langugae model based on transformer developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(b) Prepare dataset for training\n",
    "\n",
    "Please fill the code cell below to download the dataset and prepare the dataset for finetuning.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 6,
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([4, 1024])\n",
      "DataLoader is working correctly!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# your code here: load the dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# get 10% of dataset\n",
    "dataset_train = dataset[\"train\"].select(range(len(dataset[\"train\"]) // 10))\n",
    "dataset_valid = dataset[\"validation\"].select(range(len(dataset[\"validation\"]) // 10))\n",
    "\n",
    "# your code here: implement function that tokenize the dataset and set labels to be the same as input_ids\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples['text'], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "# your code here: tokenize the dataset (you may need to remove columns that are not needed)\n",
    "tokenized_datasets_train = dataset_train.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_datasets_valid = dataset_valid.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "\n",
    "tokenized_datasets_train.set_format(\"torch\")\n",
    "tokenized_datasets_valid.set_format(\"torch\")\n",
    "\n",
    "# your code here: create datacollator for training and validation dataset\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets_train, shuffle=True, batch_size=4, collate_fn=data_collator)\n",
    "valid_dataloader = DataLoader(tokenized_datasets_valid, batch_size=4, collate_fn=data_collator)\n",
    "\n",
    "# Test the DataLoader\n",
    "for batch in train_dataloader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break\n",
    "\n",
    "print(\"DataLoader is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(c) Evaluate perplexity on wiki-text\n",
    "\n",
    "Before finetuning, we evaluate the pre-trained GPT2 model on the wiki-text dataset. The perplexity is a common metric to evaluate the performance of language model. The lower the perplexity, the better the model. To compute the perplexity in practice, we use the formula as follows, which is a transformation of the formula in class:\n",
    "$PP(W) = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i|\\text{context})\\right)$"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 7,
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial perplexity: 42.9958610534668\n"
     ]
    }
   ],
   "source": [
    "def evaluate_perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # your code here: get the input_ids, attention_mask, and labels from the batch\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # your code here: forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # your code here: calculate the loss\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_length += attention_mask.sum().item()\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_length))\n",
    "    \n",
    "    return perplexity.item()\n",
    "    \n",
    "\n",
    "perplexity = evaluate_perplexity(model, valid_dataloader)\n",
    "print(f\"Initial perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(d) Fine-tune GPT2 on wiki-text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 8,
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
<<<<<<< HEAD
      "C:\\Users\\18208\\AppData\\Local\\Temp\\ipykernel_31128\\3343238000.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
=======
      "C:\\Users\\18208\\AppData\\Local\\Temp\\ipykernel_21792\\3343238000.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
<<<<<<< HEAD
       "      [1377/1377 13:47, Epoch 3/3]\n",
=======
       "      [1377/1377 14:28, Epoch 3/3]\n",
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>0.192434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.203100</td>\n",
       "      <td>0.193489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.195232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-wikitext-2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    prediction_loss_only=True,\n",
    "    # your code here: report validation and training loss every epoch\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# your code here: create a Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,                 # Your GPT-2 model\n",
    "    args=training_args,          # Training arguments\n",
    "    train_dataset=tokenized_datasets_train, # Training dataset\n",
    "    eval_dataset=tokenized_datasets_valid,  # Validation dataset\n",
    "    tokenizer=tokenizer,         # GPT-2 tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuned perplexity: 27.341890335083008\n"
     ]
    }
   ],
   "source": [
    "# your code here: load the fine-tuned model\n",
    "model_finetuned = model.from_pretrained(\"./gpt2-wikitext-2\")\n",
    "perplexity = evaluate_perplexity(model_finetuned, valid_dataloader)\n",
    "print(f\"fine-tuned perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some text using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is a langugae model based on transformers developed by OpenAI , and is based on the same basic principles as the CATAS model . The CATAS model is based on the CATAS model of the interaction between the two nuclei of the CNS . The CATAS model is based on the interaction between the CNS and the CNS + the interaction between the CNS and the CNS + the interaction between the CNS and the CNS + the interaction between the CNS and the CNS + the\n"
     ]
    }
   ],
   "source": [
    "# load the fine-tuned model\n",
    "model_finetuned = model.to('cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_finetuned.config.pad_token_id = model_finetuned.config.eos_token_id\n",
    "\n",
    "\n",
    "# generate text\n",
    "generate_text(model_finetuned, tokenizer, \"GPT-2 is a langugae model based on transformers developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2(e) Parameter efficient fine-tuning (LoRA)\n",
    "\n",
    "finetune the base gpt model through LoRA"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 11,
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\torch\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
<<<<<<< HEAD
=======
      "C:\\Users\\18208\\AppData\\Local\\Temp\\ipykernel_21792\\755661637.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
<<<<<<< HEAD
       "      [1377/1377 08:36, Epoch 3/3]\n",
=======
       "      [1377/1377 08:43, Epoch 3/3]\n",
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
<<<<<<< HEAD
       "      <td>4.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.799700</td>\n",
=======
       "      <td>4.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.289900</td>\n",
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Perplexity after lora finetuning: 30.291595458984375\n"
=======
      "Perplexity after lora finetuning: 42.94930648803711\n"
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# your code here: load GPT2 model and add the lora adapter\n",
<<<<<<< HEAD
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model_lora = get_peft_model(model, peft_config)\n",
=======
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "model_lora = get_peft_model(model, peft_config)\n",
    "model_lora.print_trainable_parameters() \n",
    "\n",
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-lora-wikitext-2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# your code here: set trainer and train the model\n",
    "trainer = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_train,\n",
    "    eval_dataset=tokenized_datasets_valid,\n",
<<<<<<< HEAD
    "    data_collator=data_collator,\n",
=======
    "    tokenizer=tokenizer,\n",
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "ppl = evaluate_perplexity(model_lora, valid_dataloader)\n",
    "print(f\"Perplexity after lora finetuning: {ppl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate lora fine-tuned model on wiki-text\n",
    "\n",
    "compare the text generated by the fully fine-tuned model and LoRA fine-tuned model and the pre-trained model. Do you see any difference in the quality of the generated text? Try to explain why. (Hint: trust your result and report as it is.)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 17,
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "GPT-2 is a langugae model based on transformers developed by OpenAI , which is based on the concept of a \"transformation-based\" model of the brain . The model is based on the concept of a \"transformation-based\" model of the brain , which is based on the concept of a \"transformation-based\" model of the brain . The model is based on the concept of a \"transformation-based\" model of the brain , which is based\n"
=======
      "GPT-2 is a langugae model based on transformers developed by OpenAI. It is a model that is based on the concept of a \"transformation\" of the human brain.\n",
      "\n",
      "The model is based on the concept of a \"transformation\" of the human brain.\n",
      "\n",
      "The model is based on the concept of a \"transformation\" of the human brain.\n"
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "generate_text(model_lora, tokenizer, \"GPT-2 is a langugae model based on transformers developed by OpenAI\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the perplexity of the fully fine-tuned model and LoRA fine-tuned model. Do you see any difference in the perplexity? Try to explain why. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 18,
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Perplexity after lora finetuning: 30.291595458984375\n"
=======
      "Perplexity after lora finetuning: 42.94930648803711\n"
>>>>>>> 7a348ab4605d1dd9c1941780caff6a1fe8e2d9e7
     ]
    }
   ],
   "source": [
    "ppl = evaluate_perplexity(model_lora, valid_dataloader)\n",
    "\n",
    "print(f\"Perplexity after lora finetuning: {ppl}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
