{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhDUFBxt9xZg"
      },
      "source": [
        "# Implement and train a LSTM for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW6ymxu99xZk"
      },
      "source": [
        "## Step 0: set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spc_UH4B9xZl",
        "outputId": "3800827c-d1e7-48ed-ae59-2531e3933c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in d:\\anaconda3\\envs\\torch\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\18208\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "\n",
        "import functools\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import os\n",
        "os.makedirs(\"resources\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEK3DN9Fwh_M"
      },
      "source": [
        "### Hyperparameters. No need to touch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "OxnFjs3f9xZn"
      },
      "outputs": [],
      "source": [
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        # Constance hyperparameters. They have been tested and don't need to be tuned.\n",
        "        self.PAD_INDEX = 0\n",
        "        self.UNK_INDEX = 1\n",
        "        self.PAD_TOKEN = '<pad>'\n",
        "        self.UNK_TOKEN = '<unk>'\n",
        "        self.STOP_WORDS = set(stopwords.words('english'))\n",
        "        self.MAX_LENGTH = 256\n",
        "        self.BATCH_SIZE = 96\n",
        "        self.EMBEDDING_DIM = 1\n",
        "        self.HIDDEN_DIM = 100\n",
        "        self.OUTPUT_DIM = 2\n",
        "        self.N_LAYERS = 1\n",
        "        self.DROPOUT_RATE = 0.0\n",
        "        self.LR = 0.001\n",
        "        self.N_EPOCHS = 5\n",
        "        self.WD = 0\n",
        "        self.SEED = 12\n",
        "        self.BIDIRECTIONAL = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XODz_aDV9xZo"
      },
      "source": [
        "## Lab 1(a) Implement your own data loader function.  \n",
        "First, you need to read the data from the dataset file on the local disk.\n",
        "Then, split the dataset into three sets: train, validation and test by 7:1:2 ratio.\n",
        "Finally return x_train, x_valid, x_test, y_train, y_valid, y_test where x represents reviews and y represent labels.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "AD7HSvM19xZp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n"
          ]
        }
      ],
      "source": [
        "def load_imdb(base_csv:str = './IMDBDataset.csv'):\n",
        "    \"\"\"\n",
        "    Load the IMDB dataset\n",
        "    :param base_csv: the path of the dataset file.\n",
        "    :return: train, validation and test set.\n",
        "    \"\"\"\n",
        "    # Add your code here.\n",
        "    df = pd.read_csv(base_csv)\n",
        "    X = df['review']\n",
        "    y = df['sentiment']\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.125, random_state = 12)\n",
        "\n",
        "\n",
        "    print(f'shape of train data is {x_train.shape}')\n",
        "    print(f'shape of test data is {x_test.shape}')\n",
        "    print(f'shape of valid data is {x_valid.shape}')\n",
        "    return x_train, x_valid, x_test, y_train, y_valid, y_test\n",
        "\n",
        "x_train, x_valid, x_test, y_train, y_valid, y_test = load_imdb()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYVH6t--9xZq"
      },
      "source": [
        "## Lab 1(b): Implement your function to build a vocabulary based on the training corpus.\n",
        "Implement the build_vocab function to build a vocabulary based on the training corpus.\n",
        "You should first compute the frequency of all the words in the training corpus. Remove the words\n",
        "that are in the STOP_WORDS. Then filter the words by their frequency (â‰¥ min_freq) and finally\n",
        "generate a corpus variable that contains a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "sugI5VoJ9xZr"
      },
      "outputs": [],
      "source": [
        "def build_vocab(x_train:list, min_freq: int=5, hparams=None) -> dict:\n",
        "    \"\"\"\n",
        "    build a vocabulary based on the training corpus.\n",
        "    :param x_train:  List. The training corpus. Each sample in the list is a string of text.\n",
        "    :param min_freq: Int. The frequency threshold for selecting words.\n",
        "    :return: dictionary {word:index}\n",
        "    \"\"\"\n",
        "    # Add your code here. Your code should assign corpus with a list of words.\n",
        "    \n",
        "    all_words = []\n",
        "    for sample in x_train:\n",
        "        sample = sample.lower()\n",
        "        sample = sample.translate(str.maketrans('', '', string.punctuation))\n",
        "        words = sample.split()\n",
        "        all_words.extend(words)\n",
        "            \n",
        "    corpus = Counter(all_words)\n",
        "    corpus = {word: freq for word, freq in corpus.items() if word.lower() not in stopwords.words('english')}\n",
        "    corpus_ = [word for word, freq in corpus.items() if freq >= min_freq]\n",
        "    # creating a dict\n",
        "    vocab = {w:i+2 for i, w in enumerate(corpus_)}\n",
        "    vocab[hparams.PAD_TOKEN] = hparams.PAD_INDEX\n",
        "    vocab[hparams.UNK_TOKEN] = hparams.UNK_INDEX\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca71G17F9xZt"
      },
      "source": [
        "## Lab 1(c): Implement your tokenize function.\n",
        "For each word, find its index in the vocabulary.\n",
        "Return a list of int that represents the indices of words in the example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "c6kj_qT69xZt"
      },
      "outputs": [],
      "source": [
        "def tokenize(vocab: dict, example: str)-> list:\n",
        "    \"\"\"\n",
        "    Tokenize the give example string into a list of token indices.\n",
        "    :param vocab: dict, the vocabulary.\n",
        "    :param example: a string of text.\n",
        "    :return: a list of token indices.\n",
        "    \"\"\"\n",
        "    # Your code here.\n",
        "    example = example.lower()\n",
        "    example = example.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = example.split()\n",
        "    unk_index = vocab.get(\"<UNK>\", 1)\n",
        "    token_indices = [vocab.get(word, unk_index) for word in tokens]\n",
        "\n",
        "    return token_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ntSo4k9xZu"
      },
      "source": [
        "## Lab 1 (d): Implement the __getitem__ function. Given an index i, you should return the i-th review and label.\n",
        "The review is originally a string. Please tokenize it into a sequence of token indices.\n",
        "Use the max_length parameter to truncate the sequence so that it contains at most max_length tokens.\n",
        "Convert the label string ('positive'/'negative') to a binary index. 'positive' is 1 and 'negative' is 0.\n",
        "Return a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "2TDgA4p79xZu"
      },
      "outputs": [],
      "source": [
        "class IMDB(Dataset):\n",
        "    def __init__(self, x, y, vocab, max_length=256) -> None:\n",
        "        \"\"\"\n",
        "        :param x: list of reviews\n",
        "        :param y: list of labels\n",
        "        :param vocab: vocabulary dictionary {word:index}.\n",
        "        :param max_length: the maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.x = list(x)\n",
        "        self.y = list(y)\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"\n",
        "        Return the tokenized review and label by the given index.\n",
        "        :param idx: index of the sample.\n",
        "        :return: a dictionary containing three keys: 'ids', 'length', 'label' which represent the list of token ids, the length of the sequence, the binary label.\n",
        "        \"\"\"\n",
        "        # Add your code here.\n",
        "\n",
        "\n",
        "        review = self.x[idx]\n",
        "        label = self.y[idx]\n",
        "\n",
        "        # Tokenize the review\n",
        "        token_ids = tokenize(self.vocab, review)\n",
        "\n",
        "        # Truncate to max_length\n",
        "        if len(token_ids) > self.max_length:\n",
        "            token_ids = token_ids[:self.max_length]\n",
        "\n",
        "        # Convert label to binary\n",
        "        binary_label = 1 if label == \"positive\" else 0\n",
        "\n",
        "        return {\n",
        "            'ids': token_ids,\n",
        "            'length': len(token_ids),\n",
        "            'label': binary_label\n",
        "        }\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "def collate(batch, pad_index):\n",
        "    batch_ids = [torch.LongTensor(i['ids']) for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_length = torch.Tensor([i['length'] for i in batch])\n",
        "    batch_label = torch.LongTensor([i['label'] for i in batch])\n",
        "    batch = {'ids': batch_ids, 'length': batch_length, 'label': batch_label}\n",
        "    return batch\n",
        "\n",
        "collate_fn = collate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zgSPYmf9xZv"
      },
      "source": [
        "## Lab 1 (e): Implement the LSTM model for sentiment analysis.\n",
        "Q(a): Implement the initialization function.\n",
        "Your task is to create the model by stacking several necessary layers including an embedding layer, a lstm cell, a linear layer, and a dropout layer.\n",
        "You can call functions from Pytorch's nn library. For example, nn.Embedding, nn.LSTM, nn.Linear.<br>\n",
        "Q(b): Implement the forward function.\n",
        "    Decide where to apply dropout.\n",
        "    The sequences in the batch have different lengths. Write/call a function to pad the sequences into the same length.\n",
        "    Apply a fully-connected (fc) layer to the output of the LSTM layer.\n",
        "    Return the output features which is of size [batch size, output dim]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "b9ofQ5R29xZv"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Embedding):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.orthogonal_(param)\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_dim: int,\n",
        "        output_dim: int,\n",
        "        n_layers: int,\n",
        "        dropout_rate: float,\n",
        "        pad_index: int,\n",
        "        bidirectional: bool = False,\n",
        "        **kwargs):\n",
        "        \"\"\"\n",
        "        Create an LSTM model for classification.\n",
        "        :param vocab_size: Size of the vocabulary.\n",
        "        :param embedding_dim: Dimension of word embeddings.\n",
        "        :param hidden_dim: Dimension of hidden features in LSTM.\n",
        "        :param output_dim: Number of output classes.\n",
        "        :param n_layers: Number of LSTM layers.\n",
        "        :param dropout_rate: Dropout rate.\n",
        "        :param pad_index: Index of the padding token.\n",
        "        :param bidirectional: Whether to use a bidirectional LSTM.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Embedding Layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=pad_index  # Ignore padding tokens in embedding\n",
        "        )\n",
        "\n",
        "        # 2. LSTM Layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,  # Shape: (batch_size, seq_len, hidden_dim)\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_rate if n_layers > 1 else 0  # Dropout only when n_layers > 1\n",
        "        )\n",
        "\n",
        "        # 3. Fully Connected (Linear) Layer\n",
        "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "        self.fc = nn.Linear(lstm_output_dim, output_dim)\n",
        "\n",
        "        # 4. Dropout Layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Initialize weights\n",
        "        if \"weight_init_fn\" not in kwargs:\n",
        "            self.apply(init_weights)\n",
        "        else:\n",
        "            self.apply(kwargs[\"weight_init_fn\"])\n",
        "\n",
        "    def forward(self, ids: torch.Tensor, length: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward pass of LSTM model.\n",
        "        :param ids: [batch_size, seq_len] Tokenized input sequences.\n",
        "        :param length: [batch_size] Length of each sequence before padding.\n",
        "        :return: [batch_size, output_dim] Predicted logits.\n",
        "        \"\"\"\n",
        "        # 1. Embedding Lookup\n",
        "        embedded = self.embedding(ids)  # Shape: [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # 2. Pack Padded Sequence (Handles variable-length sequences)\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, length.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # 3. LSTM Forward Pass\n",
        "        packed_output, (hidden, _) = self.lstm(packed_embedded)\n",
        "\n",
        "        # 4. Unpack Sequence\n",
        "        lstm_output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # 5. Extract Final Hidden State\n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # Concatenate last two hidden states for bidirectional LSTM\n",
        "        else:\n",
        "            hidden = hidden[-1]  # Take the last hidden state for unidirectional LSTM\n",
        "\n",
        "        # 6. Apply Dropout\n",
        "        dropped = self.dropout(hidden)\n",
        "\n",
        "        # 7. Fully Connected Layer\n",
        "        prediction = self.fc(dropped)  # Shape: [batch_size, output_dim]\n",
        "\n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Code (do not modify)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "13Sdl7MV9xZv"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def train(dataloader, model, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
        "        ids = batch['ids'].to(device)\n",
        "        length = batch['length']\n",
        "        label = batch['label'].to(device)\n",
        "        prediction = model(ids, length)\n",
        "        loss = criterion(prediction, label)\n",
        "        accuracy = get_accuracy(prediction, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_accs.append(accuracy.item())\n",
        "        scheduler.step()\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids, length)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def get_accuracy(prediction, label):\n",
        "    batch_size, _ = prediction.shape\n",
        "    predicted_classes = prediction.argmax(dim=-1)\n",
        "    correct_predictions = predicted_classes.eq(label).sum()\n",
        "    accuracy = correct_predictions / batch_size\n",
        "    return accuracy\n",
        "\n",
        "def predict_sentiment(text, model, vocab, device):\n",
        "    tokens = tokenize(vocab, text)\n",
        "    ids = [vocab[t] if t in vocab else UNK_INDEX for t in tokens]\n",
        "    length = torch.LongTensor([len(ids)])\n",
        "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
        "    prediction = model(tensor, length).squeeze(dim=0)\n",
        "    probability = torch.softmax(prediction, dim=-1)\n",
        "    predicted_class = prediction.argmax(dim=-1).item()\n",
        "    predicted_probability = probability[predicted_class].item()\n",
        "    return predicted_class, predicted_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnLooBJ4wh_P"
      },
      "source": [
        "### Learning rate warmup. DO NOT TOUCH!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "9rHTjuZpwh_P"
      },
      "outputs": [],
      "source": [
        "class ConstantWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer,\n",
        "        num_warmup_steps: int,\n",
        "    ):\n",
        "        self.num_warmup_steps = num_warmup_steps\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self._step_count <= self.num_warmup_steps:\n",
        "            # warmup\n",
        "            scale = 1.0 - (self.num_warmup_steps - self._step_count) / self.num_warmup_steps\n",
        "            lr = [base_lr * scale for base_lr in self.base_lrs]\n",
        "            self.last_lr = lr\n",
        "        else:\n",
        "            lr = self.base_lrs\n",
        "        return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teBvNRJWwh_P"
      },
      "source": [
        "### Implement the training / validation iteration here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "qXLkQSnS9xZw"
      },
      "outputs": [],
      "source": [
        "def train_and_test_model_with_hparams(hparams, model_type=\"lstm\", **kwargs):\n",
        "    # Seeding. DO NOT TOUCH! DO NOT TOUCH hparams.SEED!\n",
        "    # Set the random seeds.\n",
        "    torch.manual_seed(hparams.SEED)\n",
        "    random.seed(hparams.SEED)\n",
        "    np.random.seed(hparams.SEED)\n",
        "\n",
        "    x_train, x_valid, x_test, y_train, y_valid, y_test = load_imdb()\n",
        "    vocab = build_vocab(x_train, hparams=hparams)\n",
        "    vocab_size = len(vocab)\n",
        "    print(f'Length of vocabulary is {vocab_size}')\n",
        "\n",
        "    train_data = IMDB(x_train, y_train, vocab, hparams.MAX_LENGTH)\n",
        "    valid_data = IMDB(x_valid, y_valid, vocab, hparams.MAX_LENGTH)\n",
        "    test_data = IMDB(x_test, y_test, vocab, hparams.MAX_LENGTH)\n",
        "\n",
        "    collate = functools.partial(collate_fn, pad_index=hparams.PAD_INDEX)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate, shuffle=True)\n",
        "    valid_dataloader = torch.utils.data.DataLoader(\n",
        "        valid_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "    test_dataloader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=hparams.BATCH_SIZE, collate_fn=collate)\n",
        "\n",
        "    # Model\n",
        "\n",
        "    model = LSTM(\n",
        "            vocab_size,\n",
        "            hparams.EMBEDDING_DIM,\n",
        "            hparams.HIDDEN_DIM,\n",
        "            hparams.OUTPUT_DIM,\n",
        "            hparams.N_LAYERS,\n",
        "            hparams.DROPOUT_RATE,\n",
        "            hparams.PAD_INDEX,\n",
        "            hparams.BIDIRECTIONAL,\n",
        "            **kwargs)\n",
        "    \n",
        "    num_params = count_parameters(model)\n",
        "    print(f'The model has {num_params:,} trainable parameters')\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # DO NOT TOUCH optimizer-specific hyperparameters! (e.g., eps, momentum)\n",
        "    # DO NOT change optimizer implementations!\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # Start training\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    # Warmup Scheduler. DO NOT TOUCH!\n",
        "    WARMUP_STEPS = 200\n",
        "    lr_scheduler = ConstantWithWarmup(optimizer, WARMUP_STEPS)\n",
        "\n",
        "    for epoch in range(hparams.N_EPOCHS):\n",
        "\n",
        "        # Your code: implement the training process and save the best model.\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        for batch in train_dataloader:\n",
        "            ids = batch['ids'].to(device)\n",
        "            lengths = batch['length'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(ids, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "            correct = (predictions == labels).float()\n",
        "            acc = correct.sum() / len(correct)\n",
        "            train_accuracies.append(acc.item())\n",
        "\n",
        "        epoch_train_loss = np.mean(train_losses)\n",
        "        epoch_train_acc = np.mean(train_accuracies)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        valid_accuracies = []\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_dataloader:\n",
        "                ids = batch['ids'].to(device)\n",
        "                lengths = batch['length'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(ids, lengths)\n",
        "                loss = criterion(outputs, labels)\n",
        "                valid_losses.append(loss.item())\n",
        "\n",
        "                # Calculate accuracy\n",
        "                predictions = outputs.argmax(dim=1)\n",
        "                correct = (predictions == labels).float()\n",
        "                acc = correct.sum() / len(correct)\n",
        "                valid_accuracies.append(acc.item())\n",
        "\n",
        "        epoch_valid_loss = np.mean(valid_losses)\n",
        "        epoch_valid_acc = np.mean(valid_accuracies)\n",
        "\n",
        "        # Save the model that achieves the smallest validation loss.\n",
        "        if epoch_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = epoch_valid_loss\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        print(f'epoch: {epoch+1}')\n",
        "        print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
        "        print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n",
        "\n",
        "    # Load the best model's weights.\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "    # Evaluate test loss on testing dataset (NOT Validation)\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            ids = batch['ids'].to(device)\n",
        "            lengths = batch['length'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(ids, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_losses.append(loss.item())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "            correct = (predictions == labels).float()\n",
        "            acc = correct.sum() / len(correct)\n",
        "            test_accuracies.append(acc.item())\n",
        "\n",
        "    epoch_test_loss = np.mean(test_losses)\n",
        "    epoch_test_acc = np.mean(test_accuracies)\n",
        "    print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')\n",
        "\n",
        "    # Select one of the entries in test set and predict its sentiment, print out the text, prediction and the probability.\n",
        "    idx = random.randint(0, len(test_data) - 1)\n",
        "    sample = test_data[idx]\n",
        "    text = x_test.iloc[idx]\n",
        "    label = y_test.iloc[idx]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        ids = torch.LongTensor(sample['ids']).unsqueeze(0).to(device)\n",
        "        length = torch.LongTensor([sample['length']]).to(device)\n",
        "        output = model(ids, length)\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "        prediction = probs.argmax(dim=1).item()\n",
        "        probability = probs[0][prediction].item()\n",
        "\n",
        "        sentiment = 'positive' if prediction == 1 else 'negative'\n",
        "        print(f'\\nSample Text: {text}')\n",
        "        print(f'Actual Sentiment: {label}')\n",
        "        print(f'Predicted Sentiment: {sentiment}, Probability: {probability:.4f}')\n",
        "\n",
        "    # Free memory for later usage.\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    return {\n",
        "        'num_params': num_params,\n",
        "        \"test_loss\": epoch_test_loss,\n",
        "        \"test_acc\": epoch_test_acc,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKCu4rPBA2Sp"
      },
      "source": [
        "### Lab 1 (f): Train LSTM model .\n",
        "\n",
        "Train the model with default hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzatRvfMwh_Q",
        "outputId": "330f7065-2980-4791-a834-1779ebec7697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (10000,)\n",
            "shape of valid data is (5000,)\n",
            "Length of vocabulary is 38034\n",
            "The model has 79,436 trainable parameters\n",
            "epoch: 1\n",
            "train_loss: 0.689, train_acc: 0.541\n",
            "valid_loss: 0.676, valid_acc: 0.563\n",
            "epoch: 2\n",
            "train_loss: 0.558, train_acc: 0.716\n",
            "valid_loss: 0.389, valid_acc: 0.835\n",
            "epoch: 3\n",
            "train_loss: 0.314, train_acc: 0.871\n",
            "valid_loss: 0.363, valid_acc: 0.850\n",
            "epoch: 4\n",
            "train_loss: 0.220, train_acc: 0.917\n",
            "valid_loss: 0.349, valid_acc: 0.873\n",
            "epoch: 5\n",
            "train_loss: 0.172, train_acc: 0.940\n",
            "valid_loss: 0.401, valid_acc: 0.862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\18208\\AppData\\Local\\Temp\\ipykernel_22964\\325452999.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pt'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_loss: 0.320, test_acc: 0.873\n",
            "\n",
            "Sample Text: Deanna Durbin really did save Universal from bankruptcy and enabled it to remain a big studio. By the mid 30s most of the big directors that had been at Universal eg Milestone, Browning and Wyler had gone. Only James Whale remained but his prestigious horror films were behind him. Deanna and Judy Garland appeared in a short \"Every Sunday\" and initially Garland was suggested for the role of Penny in \"Three Smart Girls\". When Garland was unavailable Universal switched to Durbin. Initially she had been definitely a supporting player but her potential was so vivid that the script was rewritten to make her the star. Directed by Henry Koster the film had a European touch.<br /><br />The film starts with a beautiful panorama of a lake in \"Switzerland\". The \"three smart girls\" of the title - three sisters, Joan (Nan Grey), Kay (Barbara Read) and Penny (Deanna Durbin) are sailing with Penny giving her glorious voice to \"My Heart is Singing\". All is not too well on the home front - their father is planning to remarry a younger woman (Binnie Barnes) so the three girls with the help of their trusty nurse (Lucille Watson) decide to go to New York and reunite him with their mother. Lucille Watson is best remembered for her role as Robert Taylor's stern mother in \"Waterloo Bridge\" (1941).<br /><br />Donna is a gold-digger who, along with her scatty mother (Alice Brady), is determined to marry Judson Craig (Charles Winninger). For someone with no film experience Deanna is wonderful as Penny, a typical pesky, over enthusiastic kid sister and she is as pretty as a picture. When she sings \"Someone to Care for Me\" to her father you will just melt - what a glorious voice she had. She also has one of the funniest lines in the film. When her father consoles her with \"I'll take you to the zoo tomorrow\", she replies \"Oh I can see enough monkeys around here\"!!!<br /><br />With the help of Bill Evans (John King) they decide to hire a \"count\" (Mischa Auer)to romance Donna. They arrange to meet at a nightclub but due to a mix-up Lord Michael Stuart (Ray Milland) is mistaken for the count and Donna falls for him (she thinks he owns half of Australia!!!) The plan backfires as he falls for Kay and Donna wants to hasten her marriage to Judson.<br /><br />Penny decides to take matters into her own hands and runs away. She is taken to the local police station where she charms the cops with her rendition of \"Il Bacio\" (she is trying to convince them she is a young opera singer.) Everything ends happily with their mother (Nella Walker) sailing over to patch things up with their dad and in the meantime Donna makes the acquaintance of the phoney count (Mischa Auer) and sails off to Australia with him.<br /><br />Highly recommended.\n",
            "Actual Sentiment: positive\n",
            "Predicted Sentiment: positive, Probability: 0.9957\n"
          ]
        }
      ],
      "source": [
        "org_hyperparams = HyperParams()\n",
        "_ = train_and_test_model_with_hparams(org_hyperparams, \"lstm_1layer_base_adam_e32_h100\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
